{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23UqqhL6n-Lb",
        "outputId": "7cc1eba2-85be-4c82-cfbd-0d2a3b63b3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s455QBKynlBc"
      },
      "source": [
        "## 1. 서울 + 결측치\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygAmYyM4novR"
      },
      "source": [
        "## 2. 열제거 (PM10, PM2.5 제외 제거하기)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUe1EHSfnvYI"
      },
      "source": [
        "## 3. average of 11am , 12pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1UX1TUn02p"
      },
      "source": [
        "## 4. 지오코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\uaua1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "한 연도 안에서 지역 분류, 결측치 제거 + PM 값은 10시와 11시 값의 평균값 구해서 넣기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 5개년 데이터를 저장한 폴더 경로\n",
        "project_folder = \"./에어코리아\"\n",
        "address_file = \"./기상관측데이터/기상관측데이터_지점정리.csv\"\n",
        "output_folder = \"./에어코리아\"\n",
        "address = pd.read_csv(address_file)\n",
        "location_names = address['지점명'].dropna().unique()\n",
        "\n",
        "# 결과 저장용 데이터프레임 초기화\n",
        "all_results = pd.DataFrame()\n",
        "\n",
        "# 연도별 데이터 처리\n",
        "for year_folder in ['2019', '2020', '2021', '2022', '2023']:\n",
        "    year_folder_path = os.path.join(project_folder, year_folder)\n",
        "\n",
        "    if os.path.exists(year_folder_path):\n",
        "        print(f\"Processing folder: {year_folder_path}\")\n",
        "\n",
        "        for filename in os.listdir(year_folder_path):\n",
        "            if filename.endswith(\".xlsx\"):\n",
        "                file_path = os.path.join(year_folder_path, filename)\n",
        "                df = pd.read_excel(file_path, engine='openpyxl')\n",
        "\n",
        "                for location_name in location_names:\n",
        "                    # 특정 지점명으로 데이터 필터링\n",
        "                    df_filtered = df[df['주소'].str.contains(location_name, na=False)]\n",
        "                    if df_filtered.empty:\n",
        "                        continue\n",
        "\n",
        "                    # 새로운 열에 지점명 추가\n",
        "                    # 수정된 코드\n",
        "                    df_filtered.loc[:, '지점명'] = location_name\n",
        "                    \n",
        "                    # 결측치 제거 (PM10 또는 PM2.5 값이 NaN인 행 제거)\n",
        "                    df_filtered = df_filtered.dropna(subset=['PM10', 'PM25'])\n",
        "\n",
        "                    # '측정일시'를 문자열로 변환 후 시간 필터링\n",
        "                    df_filtered['측정일시'] = df_filtered['측정일시'].astype(str)\n",
        "                    df_filtered['시간'] = df_filtered['측정일시'].str[-2:].astype(int)\n",
        "\n",
        "                    # 10시와 11시 데이터만 선택\n",
        "                    df_filtered = df_filtered[df_filtered['시간'].isin([10, 11])]\n",
        "                    \n",
        "                    # 측정일시에서 날짜만 추출\n",
        "                    df_filtered['측정일시_날짜'] = df_filtered['측정일시'].str[:8]\n",
        "\n",
        "                    # 불필요한 열 제거\n",
        "                    drop_columns = ['지역', '망', '측정소명', 'SO2', 'CO', 'O3', 'NO2']\n",
        "                    df_filtered = df_filtered.drop(columns=[col for col in drop_columns if col in df_filtered.columns])\n",
        "\n",
        "                    # 평균 계산\n",
        "                    grouped_df = df_filtered.groupby(\n",
        "                        ['지점명', '측정소코드', '측정일시_날짜', '주소']\n",
        "                    )[['PM10', 'PM25']].mean().reset_index()\n",
        "\n",
        "                    # 결과 데이터프레임에 추가\n",
        "                    all_results = pd.concat([all_results, grouped_df], ignore_index=True)\n",
        "\n",
        "# 동일 연도 데이터끼리 파일로 저장\n",
        "all_results['측정일시_년도'] = all_results['측정일시_날짜'].str[:4]\n",
        "\n",
        "for year, year_data in all_results.groupby('측정일시_년도'):\n",
        "    output_file = os.path.join(output_folder, f\"{year}_data.csv\")\n",
        "    year_data.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"작업이 완료되었습니다!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터프레임에 결측치가 없습니다.\n"
          ]
        }
      ],
      "source": [
        "if all_results.isna().any().any():\n",
        "    print(\"데이터프레임에 결측치가 존재합니다.\")\n",
        "else:\n",
        "    print(\"데이터프레임에 결측치가 없습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "각 지역 별로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지점명별 연도별 CSV 파일 생성이 완료되었습니다!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 데이터 폴더 경로\n",
        "input_folder = \"./에어코리아\"\n",
        "output_folder = \"./에어코리아\"\n",
        "\n",
        "# 기상관측소 파일 로드\n",
        "station_file = \"./기상관측데이터/기상관측데이터_지점정리.csv\"\n",
        "station_data = pd.read_csv(station_file, encoding='utf-8-sig')\n",
        "\n",
        "# '지점명'과 '주소' 매핑 테이블 생성\n",
        "station_names = station_data['지점명'].dropna().unique()  # 유니크한 지점명만 활용\n",
        "\n",
        "# 연도별로 데이터 분리 및 처리\n",
        "for year_file in os.listdir(input_folder):\n",
        "    if year_file.endswith(\".csv\"):\n",
        "        # 연도 가져오기 (예: 2019_data.csv -> 2019)\n",
        "        year = year_file.split(\"_\")[0]\n",
        "        \n",
        "        # 연도별 데이터 읽기\n",
        "        file_path = os.path.join(input_folder, year_file)\n",
        "        year_data = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "        \n",
        "        # 각 지점명별로 데이터 분리\n",
        "        for station_name in station_names:\n",
        "            # 주소에 지점명이 포함된 데이터 필터링\n",
        "            group = year_data[year_data['주소'].str.contains(station_name, na=False)]\n",
        "            \n",
        "            if group.empty:\n",
        "                continue  # 해당 지점명이 포함된 데이터가 없으면 건너뛰기\n",
        "            \n",
        "            # 출력 폴더 생성 (지점명별 폴더)\n",
        "            location_folder = os.path.join(output_folder, station_name)\n",
        "            os.makedirs(location_folder, exist_ok=True)\n",
        "            \n",
        "            # 파일 이름: 연도.csv (예: 2019.csv)\n",
        "            output_file = os.path.join(location_folder, f\"{year}.csv\")\n",
        "            \n",
        "            # 데이터 저장\n",
        "            group.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"지점명별 연도별 CSV 파일 생성이 완료되었습니다!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "바로 아래 코드는 csv 파일 정리를 하느라, 특정 열 없애는 데에 쓴 거. 다른 때에는 필요 없음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 지역 폴더들이 모여 있는 상위 폴더 경로\n",
        "base_folder = \"./에어코리아/지역별로_폴더분류\"\n",
        "\n",
        "# 모든 하위 폴더 탐색\n",
        "for root, dirs, files in os.walk(base_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):  # CSV 파일만 처리\n",
        "            file_path = os.path.join(root, file)\n",
        "            \n",
        "            # CSV 파일 읽기\n",
        "            df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "            \n",
        "            # '지점명' 열이 존재하면 삭제\n",
        "            if '측정일시_년도' in df.columns:\n",
        "                df = df.drop(columns=['측정일시_년도'])\n",
        "                print(f\"'측정일시_년도' 열 삭제: {file_path}\")\n",
        "                \n",
        "                # 수정된 데이터 다시 저장\n",
        "                df.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"모든 CSV 파일에서 '측정일시_년도' 열 삭제 완료!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 아래는 안 건들임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNUpEFuEn4AH"
      },
      "source": [
        "## 5. 센티넬이랑 matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRlN84XAAD_9",
        "outputId": "1970a190-664c-46f8-c6f2-673c8ca66c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230815.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230909.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230924.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231029.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231113.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231118.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231123.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231128.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231203.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231208.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231223.csv\n",
            "Filtered data saved to /content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231228.csv\n"
          ]
        }
      ],
      "source": [
        "# 미리 처리된 데이터프레임 로드\n",
        "df = pd.read_csv('/content/drive/MyDrive/대기오염 Data/New_airkorea/preprocessing/2023_pre.csv')\n",
        "\n",
        "# 새로운 specific_dates 리스트\n",
        "specific_dates = [\n",
        "    '20230815', '20230909', '20230924', '20231029', '20231113',\n",
        "    '20231118', '20231123', '20231128', '20231203', '20231208',\n",
        "    '20231223', '20231228',\n",
        "]\n",
        "\n",
        "# 새로운 파일 경로 리스트\n",
        "filtered_file_paths = [\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230815.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230909.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20230924.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231029.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231113.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231118.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231123.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231128.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231203.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231208.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231223.csv',\n",
        "    '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching/20231228.csv',\n",
        "]\n",
        "\n",
        "# Loop through each specific date and corresponding file path\n",
        "for i, specific_date in enumerate(specific_dates):\n",
        "    # Filter the dataframe by the specified date (YYYYMMDD)\n",
        "    filtered_df = df[df['측정일시_날짜'].astype(str).str[:8].isin([specific_date])]\n",
        "\n",
        "    # Save the filtered dataframe to a new CSV file\n",
        "    filtered_df.to_csv(filtered_file_paths[i], index=False)\n",
        "\n",
        "    # Print confirmation\n",
        "    print(f\"Filtered data saved to {filtered_file_paths[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEg82v84wLtY"
      },
      "source": [
        "## 6. 센티넬이랑 합치기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqZhLsKbW13S",
        "outputId": "9452b8de-edbc-4d63-8297-ba2461a5b290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed and saved 20230706.csv\n",
            "Processed and saved 20231113.csv\n",
            "Processed and saved 20230427.csv\n",
            "Processed and saved 20230201.csv\n",
            "Processed and saved 20231223.csv\n",
            "Processed and saved 20230303.csv\n",
            "Processed and saved 20230402.csv\n",
            "Processed and saved 20231228.csv\n",
            "Processed and saved 20231208.csv\n",
            "Processed and saved 20230112.csv\n",
            "Processed and saved 20230417.csv\n",
            "Processed and saved 20231118.csv\n",
            "Processed and saved 20231123.csv\n",
            "Processed and saved 20230517.csv\n",
            "Processed and saved 20230701.csv\n",
            "Processed and saved 20230815.csv\n",
            "Processed and saved 20230522.csv\n",
            "Processed and saved 20230412.csv\n",
            "Processed and saved 20230721.csv\n",
            "Processed and saved 20230313.csv\n",
            "Processed and saved 20230226.csv\n",
            "Processed and saved 20230127.csv\n",
            "Processed and saved 20230616.csv\n",
            "Processed and saved 20230407.csv\n",
            "Processed and saved 20231029.csv\n",
            "Processed and saved 20230909.csv\n",
            "Processed and saved 20231128.csv\n",
            "Processed and saved 20230221.csv\n",
            "Processed and saved 20230924.csv\n",
            "Processed and saved 20230102.csv\n",
            "Processed and saved 20231203.csv\n",
            "Processed and saved 20230805.csv\n",
            "All files processed.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 파일 경로 설정\n",
        "airkorea_folder = '/content/drive/MyDrive/대기오염 Data/New_airkorea/Sentinel-matching'\n",
        "sentinel_folder = '/content/drive/MyDrive/대기오염 Data/Sentinel 2 Data/band value/2023/'\n",
        "output_folder = '/content/drive/MyDrive/대기오염 Data/Sentinel+Airkorea'\n",
        "\n",
        "# 파일 목록 가져오기\n",
        "airkorea_files = [f for f in os.listdir(airkorea_folder) if f.endswith('.csv')]\n",
        "sentinel_files = [f for f in os.listdir(sentinel_folder) if f.endswith('.csv')]\n",
        "\n",
        "# 공통 파일 목록 찾기\n",
        "common_files = set(airkorea_files) & set(sentinel_files)\n",
        "\n",
        "# 병합 기준 컬럼 설정\n",
        "merge_columns = ['Latitude', 'Longitude']\n",
        "\n",
        "# 파일 처리\n",
        "for file_name in common_files:\n",
        "    try:\n",
        "        # 파일 읽기\n",
        "        df_airkorea = pd.read_csv(os.path.join(airkorea_folder, file_name), index_col=0)\n",
        "        df_sentinel = pd.read_csv(os.path.join(sentinel_folder, file_name))\n",
        "\n",
        "        # 데이터 병합\n",
        "        df_merged = pd.merge(df_airkorea, df_sentinel[merge_columns + ['geometry', 'AOT','Band1', 'Band2', 'Band3', 'Band4', 'Band5', 'Band6', 'Band7', 'Band11', 'Band12', 'B8A(NIR)', 'SCL', 'TCI', 'WVP']],\n",
        "                             on=merge_columns, how='left')\n",
        "        # 데이터 병합\n",
        "        #df_merged = pd.merge(df_airkorea, df_sentinel[merge_columns + ['geometry', 'AOT','Band2', 'Band3', 'Band4', 'Band5', 'Band6', 'Band7', 'Band11', 'Band12', 'B8A(NIR)', 'SCL', 'TCI', 'WVP']],\n",
        "        #                     on=merge_columns, how='left')\n",
        "\n",
        "        # 병합된 데이터프레임을 저장\n",
        "        df_merged.to_csv(os.path.join(output_folder, file_name), index=False)\n",
        "\n",
        "        # 병합 결과 출력 (첫 몇 줄)\n",
        "        print(f\"Processed and saved {file_name}\")\n",
        "\n",
        "    except KeyError as e:\n",
        "        # 오류 발생 시 메시지 출력 후 넘어가기\n",
        "        print(f\"KeyError: {e} in file {file_name}. Skipping this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # 기타 오류 발생 시 메시지 출력 후 넘어가기\n",
        "        print(f\"Error: {e} in file {file_name}. Skipping this file.\")\n",
        "\n",
        "print(\"All files processed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y20AlqXUXa8o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
